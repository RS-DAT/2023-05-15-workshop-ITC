{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f35008-7b45-4376-a6d0-addedf3441bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import pystac\n",
    "import rioxarray\n",
    "import stac2dcache\n",
    "import xarray as xr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7e953cc-1a7a-48a6-b059-8f10cf09dc34",
   "metadata": {},
   "source": [
    "# Spring Index Models from Daymet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fed32faa-8d81-4e2e-a35f-a1d9f4075b5d",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9aca26e3-c384-447e-b28e-804c2f4e53a6",
   "metadata": {},
   "source": [
    "### 1.1 Overview\n",
    "\n",
    "In this notebook we calculate spring onset indicators, namely **the day of first leaf appearance**, as 1-km gridded estimates over the conterminous United States (CONUS). As input data, we use variables from the [Daymet dataset](https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1840), which we have previously retrieved to the [SURF dCache storage](http://doc.grid.surfsara.nl/en/stable/Pages/Service/system_specifications/dcache_specs.html) in the form of a [SpatioTemporal Asset Catalog](https://stacspec.org/) (see [this notebook](https://github.com/RS-DAT/JupyterDask-Examples/blob/main/03-phenology/notebooks/01-download-Daymet4.ipynb)). The same storage system is used for the output spring index products, which we save in [Zarr](https://zarr.readthedocs.io/en/stable/) format. This work is based on the publication [Izquierdo-Veriguier et al., 2018](https://doi.org/10.1016/j.agrformet.2018.06.028). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86b64455-e65c-4961-ac7b-194f6b13285a",
   "metadata": {},
   "source": [
    "### 1.2 The model\n",
    "\n",
    "The first-leaf spring indices have been computed following the Extended Spring Index (SI-x) models from [Schwartz et al., 2013](https://doi.org/10.1002/joc.3625). Input data variables, taken from the Daymet dataset, are the daily minimum and maximum temperatures and the daylight duration. \n",
    "\n",
    "Using the SI-x models, the first-leaf dates are estimated for the *Lilac* plant species. For more information have a look at the original publication [Izquierdo-Veriguier, 2018](\n",
    "https://doi.org/10.1016/j.agrformet.2018.06.028)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede9e63c-7144-455d-9827-8333244f50b0",
   "metadata": {},
   "source": [
    "### 1.3 Before running this notebook\n",
    "\n",
    "The input and output datasets as well as the corresponding  metadata are stored on the SURF dCache system, which we access via bearer-token authentication with a macaroon. The macaroon, generated using [this script](https://github.com/sara-nl/GridScripts/blob/master/get-macaroon), is stored together with other configuration parameters within a JSON fsspec configuration file (also see the [STAC2dCache tutorial](https://github.com/NLeSC-GO-common-infrastructure/stac2dcache/blob/main/notebooks/tutorial.ipynb) and the [fsspec documentation](https://filesystem-spec.readthedocs.io/en/latest/features.html#configuration) for more info)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "619a469b-b1e7-4a29-99f9-22e5c1e5d7ad",
   "metadata": {},
   "source": [
    "## 2. Calculating the Spring Indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f85c0109-c5ed-4b08-a216-c800a402fae5",
   "metadata": {},
   "source": [
    "### 2.1 Overview\n",
    "\n",
    "The calculation of the spring index events involves the following steps: \n",
    "* opening the input variables from the retrieved collection; \n",
    "* performing some preprocessing operations (filtering the spatial and temporal extents from the daily records, carrying out few conversions);\n",
    "* estimating the spring index dates on the 1-km grid on which input variables are provided;\n",
    "* saving the output.\n",
    "\n",
    "All the steps are run by looping over years and by using a [Dask](http://dask.org) cluster to parallelize operations over spatial regions and days of the year. For the purpose of this demo we run the spring index calculation for a single year - see [this notebook](https://github.com/RS-DAT/JupyterDask-Examples/blob/main/03-phenology/notebooks/02-compute-spring-index.ipynb) for an example run involving the full time span of the Daymet dataset (42 years)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d9795ab-0281-4546-97f8-94465acec3c8",
   "metadata": {},
   "source": [
    "### 2.2 Input parameters  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41fb36f0-1cd2-4a8e-9a9b-08ccc4a464ec",
   "metadata": {},
   "source": [
    "The following variables define the parameters for the spring index calculations. These include the range of years, the range of days where to look for the spring onset events, and the boundaries of the area of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f2e37-ceb8-4931-8af0-f09478c64987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one year to compute the spring index \n",
    "year = 1980\n",
    "\n",
    "# Year day range for calculating growing degree hours\n",
    "startdate = 1 \n",
    "enddate = 300\n",
    "\n",
    "# Bounding box expressed in lat/lon degrees\n",
    "bbox_latlon = (-124.784, 24.743, -66.951, 49.346)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e2c4a63-b85d-45e7-96eb-8790060ec676",
   "metadata": {},
   "source": [
    "We also set the dCache path to the STAC catalog where we have archived the Daymet dataset and the path where to store the output spring indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b3b53-7a13-42fb-ba1f-12f72e4e24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dCache project root path\n",
    "root_urlpath = \"dcache://pnfs/grid.sara.nl/data/remotesensing/disk\"\n",
    "\n",
    "catalog_urlpath = f\"{root_urlpath}/daymet-daily-v4/catalog.json\"\n",
    "output_urlpath = f\"{root_urlpath}/demo/spring-index-models.zarr\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fab83359-ab2d-4e1c-be33-b249894e1ee1",
   "metadata": {},
   "source": [
    "### 2.3 The model\n",
    "\n",
    "The SI-x model is encoded in the following few functions, which are used to calculate the first-leaf spring index dates. From the input variables extracted from Daymet, the growing degree hours (GDH) is first computed. A set of predictors is then calculated from the GDH, and these are in turn used to estimate the spring onset dates for our plant species.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14971e1-a137-4ab1-abeb-589e06c816de",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TEMP_FAHRENHEIT = 31.\n",
    "\n",
    "HOURS = xr.DataArray(\n",
    "    data=da.arange(24), \n",
    "    dims=(\"hours\",),\n",
    ")\n",
    "\n",
    "DAYS = xr.DataArray(\n",
    "    data=da.arange(startdate, enddate+1),\n",
    "    dims=(\"time\",),\n",
    ")\n",
    "\n",
    "LEAF_INDEX_COEFFS = xr.DataArray(\n",
    "    data=da.from_array(\n",
    "        [\n",
    "            [3.306, 13.878, 0.201, 0.153],\n",
    "        ],\n",
    "        chunks=(1,-1)\n",
    "    ),\n",
    "    dims=(\"plant\", \"variable\"),\n",
    "    coords={\"plant\": [\"lilac\"]}\n",
    ")\n",
    "\n",
    "LEAF_INDEX_LIMIT = 637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae365b-12b2-4ac8-9e2f-cf8a78168894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gdh(dayl, tmin, tmax):\n",
    "    \"\"\" \n",
    "    Calculate growing degree hours (GDH). \n",
    "    \"\"\"\n",
    "    \n",
    "    dt = tmax - tmin\n",
    "    const = np.sin(np.pi/(dayl + 4) * dayl) * dt\n",
    "    \n",
    "    eq1 = np.sin(HOURS * np.pi/(dayl + 4)) * dt \n",
    "    eq2 = (1 - np.log(HOURS - np.floor(dayl))/np.log(24 - dayl)) * const\n",
    "    t = xr.where(~np.isfinite(eq2), eq1, eq2) + tmin - BASE_TEMP_FAHRENHEIT\n",
    "    t = t.clip(min=0)\n",
    "    return t.sum(dim=\"hours\", skipna=False)\n",
    "\n",
    "\n",
    "def calculate_leaf_predictors(gdh):\n",
    "    \"\"\"\n",
    "    Calculate predictors for first leaf: DDE2, DD57, MDS0, and SYNOP.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pad GDH to solve issues with first days of the year\n",
    "    gdh_padded = gdh.pad(time=(7,0), mode=\"edge\")\n",
    "    \n",
    "    # Calculating dde2 - trailing 3 days GDH sum from day i-2 to i\n",
    "    dde2 = gdh_padded.rolling(time=3, center=False).sum()\n",
    "    dde2 = dde2.isel(time=slice(7, None))  # drop padded values \n",
    "    \n",
    "    # Calculating dd57 - trailing 5-7 days GDH sum from day i-7 to i-5\n",
    "    dd57 = gdh_padded.rolling(time=8, center=False).sum() \\\n",
    "        - gdh_padded.rolling(time=5, center=False).sum()\n",
    "    dd57 = dd57.isel(time=slice(7, None))  # drop padded values\n",
    "    \n",
    "    # Calculating mds0\n",
    "    mds0 = DAYS - 1\n",
    "    \n",
    "    # Calculating synop\n",
    "    synflag = dde2>=LEAF_INDEX_LIMIT\n",
    "    synop = synflag.cumsum(dim=\"time\")\n",
    "\n",
    "    return dde2, dd57, mds0, synop\n",
    "\n",
    "\n",
    "def calculate_first_leaf(dde2, dd57, mds0, synop):\n",
    "    \"\"\"\n",
    "    Calculate day of first leaf for each plant species from GDH.\n",
    "    \"\"\" \n",
    "            \n",
    "    # Prediction calculation for first leaf\n",
    "    mdsum = LEAF_INDEX_COEFFS[:,0]*mds0 \\\n",
    "        + LEAF_INDEX_COEFFS[:,1]*synop \\\n",
    "        + LEAF_INDEX_COEFFS[:,2]*dde2 \\\n",
    "        + LEAF_INDEX_COEFFS[:,3]*dd57\n",
    "\n",
    "    mdbool = mdsum>999.5  # Calculate all occurences of first leaf\n",
    "\n",
    "    # Vectorized approach to identifying first day of leaf\n",
    "    outdate = mdbool.argmax(dim=\"time\")\n",
    "    outdate = outdate.where(mdbool.sum(dim=\"time\")>0)\n",
    "            \n",
    "    return outdate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8920d33-1efa-4f4f-b8e1-5e3f757da827",
   "metadata": {},
   "source": [
    "### 2.4 Open the input catalog \n",
    "\n",
    "The input variables (minimum temperature, maximum temperature and day length duration) are extracted from the Daymet catalog, which we have dowloaded earlier as a STAC catalog (see [this notebook](./01-download-Daymet4.ipynb)). In order to get access to the data we load the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57bdf4-d7be-4928-a1e5-de41e4c6aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac.Catalog.from_file(catalog_urlpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f95ab43-3288-4dbd-b6f0-f7862764333a",
   "metadata": {},
   "source": [
    "In addition to providing links to the data, the catalog provides all the dataset's metadata, which we use e.g. to convert the bounding box from latitude/logitude degrees to the dataset's coordinate reference system (CRS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d2c62-587a-4abe-a164-636b7007b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information about input CRS from metadata\n",
    "_item = next(catalog.get_all_items())\n",
    "proj_json = _item.properties[\"proj:projjson\"]\n",
    "crs_lcc = pyproj.CRS.from_json_dict(proj_json)\n",
    "\n",
    "# Set up CRS converter\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    crs_from=\"EPSG:4326\", \n",
    "    crs_to=crs_lcc,\n",
    "    always_xy=True,\n",
    ")\n",
    "\n",
    "# Calculate bbox in the dataset's CRS\n",
    "bbox = transformer.transform_bounds(*bbox_latlon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb8a307-136f-4039-a39b-36e250e6fdb0",
   "metadata": {},
   "source": [
    "### 2.5 Connect to the cluster\n",
    "\n",
    "Once we are ready to run the calculation we setup a Dask cluster and create a client connection. This is most easily achieved via the Dask JupyterLab extension (look for the Dask logo on the left tab of the JupyterLab interface). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c7e2924-4773-4b1a-8cfd-843ad927572b",
   "metadata": {},
   "source": [
    "*--DROP DASK `SLURMCluster` HERE--*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b111b896-b767-4b37-89ea-970449568552",
   "metadata": {},
   "source": [
    "Here we create a cluster with 15 nodes. Let's wait for all workers to join the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdc239-66f8-4362-946a-6db0fd021392",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f026a8-0691-4c3e-8b28-1282a6bd2f76",
   "metadata": {},
   "source": [
    "### 2.6 Run the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b204b92-f5fb-4709-bfdd-41d89b993596",
   "metadata": {},
   "source": [
    "Once the Dask cluster is reachable, we can start the computation! We define few convenience functions to open the dataset using the Xarray library, preprocess the input variables and save the output products to the storage. Note that by setting the size of the data \"chunks\" when reading the data, we choose to use Dask arrays as underlying data structure. All calls to Xarray's objects are then lazily executed until data are written to disk, which triggers the calculation of the spring index for a given year."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8a48e2d-bbd2-4a20-a5e4-f7064f4c2953",
   "metadata": {},
   "source": [
    "We now compute the spring indices for the selected year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de1162-6fd3-4c8b-bf1b-b219fb79fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset(urlpaths, **kwargs):\n",
    "    \"\"\"\n",
    "    Open the remote files as a single dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    ofs = fsspec.open_files(urlpaths, block_size=4*2**20)\n",
    "    return xr.open_mfdataset(\n",
    "        [of.open() for of in ofs],\n",
    "        engine=\"h5netcdf\", \n",
    "        decode_coords=\"all\",\n",
    "        drop_variables=(\"lat\", \"lon\"),\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748b3c1-6103-4a7d-b0b4-df99f73e0794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract urlpaths to Daymet files from catalog\n",
    "item = catalog.get_item(f\"na-{year}\", recursive=True)\n",
    "hrefs = [\n",
    "    item.assets[var].get_absolute_href() \n",
    "    for var in (\"tmin\", \"tmax\", \"dayl\")\n",
    "]\n",
    "    \n",
    "# Open files as a single dataset, using a chunked Dask array\n",
    "ds = open_dataset(hrefs, chunks={\"time\": 5, \"x\": 1000, \"y\": 1000})\n",
    "ds = ds.coarsen({\"x\": 20, \"y\":20}, boundary=\"trim\").mean()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f6c1b-765e-41a6-8eeb-1ae8499b3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a slice of the dataset\n",
    "ds[\"tmax\"].isel(time=0).plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc227fa9-7d29-446b-9b00-54a5a7978288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(ds, startdate, enddate, bbox):\n",
    "    \"\"\"\n",
    "    Subset the input dataset and make necessary conversions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select time range for GDH calculation\n",
    "    ds = ds.isel(time=slice(startdate-1, enddate))\n",
    "    \n",
    "    # Spatial selection\n",
    "    ds = ds.rio.clip_box(*bbox)\n",
    "    \n",
    "    # Convert temperatures to Fahrenheit\n",
    "    tmax = ds[\"tmax\"] * 1.8 + 32\n",
    "    tmin = ds[\"tmin\"] * 1.8 + 32\n",
    "\n",
    "    # Convert daylength from seconds to hours\n",
    "    dayl = ds[\"dayl\"] / 3600\n",
    "\n",
    "    return tmax, tmin, dayl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32df13f-2d87-4303-8d10-edb05930091f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract temporal/spatial ranges, unit conversion\n",
    "tmax, tmin, dayl = preprocess_dataset(ds, startdate, enddate, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060fb53-fdb6-44ba-a078-71e35c117557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot same slice after pre-processing\n",
    "tmax.isel(time=0).plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438712b-6a89-427b-a33d-d99d0d555d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate GDH and rechunk to have single chunk along time axis\n",
    "gdh = calculate_gdh(dayl, tmin, tmax)\n",
    "gdh = gdh.chunk({\"time\": enddate-startdate+1, \"x\": 500, \"y\": 500})\n",
    "gdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab6352-5658-4d27-86f4-f4e10f646298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GDH vs time for one of the pixels\n",
    "gdh.isel(x=2800, y=1500).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a66fa-9549-4f74-b50a-c115e9db663b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fist leaf index\n",
    "dde2, dd57, mds0, synop = calculate_leaf_predictors(gdh)\n",
    "first_leaf = calculate_first_leaf(dde2, dd57, mds0, synop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ffd93-2e1b-426c-893e-c96b81c0a5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trigger calculation of spring indices\n",
    "first_leaf = client.persist(first_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a208d-e4f4-4df0-9874-dbc9c9ee9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first-leaf date\n",
    "first_leaf.plot.imshow(col=\"plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74404914-4213-4f93-88c6-c6d3d125ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_urlpath(first_leaf, urlpath, group):\n",
    "    \"\"\"\n",
    "    Save output to urlpath in Zarr format. \n",
    "    \"\"\"\n",
    "    \n",
    "    fs_map = fsspec.get_mapper(urlpath)\n",
    "    ds = xr.Dataset({\n",
    "        f\"first-leaf\": first_leaf, \n",
    "    })\n",
    "    ds.to_zarr(fs_map, mode=\"w\", group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd77ec5-a4c8-4472-b18f-b4173b4149c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rechunk and save to storage\n",
    "save_to_urlpath(\n",
    "    first_leaf.chunk({\"plant\": 1, \"x\": 1000, \"y\": 1000}),\n",
    "    output_urlpath, \n",
    "    f\"{year}\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb7d0c08-267a-49c2-a931-bd9e66d7a4fd",
   "metadata": {},
   "source": [
    "While this run involved a single year for demo purposes, we have run the spring index calculation for the whole time span of the Daymet dataset for North America (42 years) in [this notebook](https://github.com/RS-DAT/JupyterDask-Examples/blob/main/03-phenology/notebooks/02-compute-spring-index.ipynb). Using 15 nodes (60 cores), the overall wall time was ~5 hours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddb8b4ee-dd6e-4f8e-a6e1-ac304f512761",
   "metadata": {},
   "source": [
    "When done, we shutdown the cluster to release resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
