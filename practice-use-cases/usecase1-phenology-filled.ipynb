{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE CASE 1: Spring Index Calculation for Puerto Rico\n",
    "\n",
    "## Description:\n",
    "\n",
    "In this practice use-case, you will use the tools learnt to calculate the spring indices for region of Puerto Rico. As you progress in the course, you will be able to complete different sections of this practice use-case to get you familiarised with the different tools. The calculations are intended to be performed using the Deployable Analysis Environment (DAT) on the infrastructure provided. This notebook will continue to be available to you after the summer school. \n",
    "\n",
    "In this practice use-case, you will perform a similar calculation for Puerto Rico as what was shown for North America in the code-along session. The workflow is split in two parts:\n",
    "\n",
    "1) Part 1 covers the downloading of the required data-set for Puerto Rico and storing it in the STAC catalog already created in the code-along session. \n",
    "\n",
    "2) Part 2 covers the calculation of the leaf and bloom spring indices for three different plant species in Puerto Rico.\n",
    "\n",
    "## Notes:\n",
    "\n",
    "This notebook will guide you through the process of setting up your data download and corresponding calculations. Some parts of the notebook are prefilled in order to help you will the calculations. Other parts of the notebook, left empty, are to be completed by you."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the required data-sets\n",
    "### ***PLEASE NOTE THAT THIS FIRST PART WILL ONLY WORK AFTER YOU HAVE RUN THE 01-data-access-and-retrieval NOTEBOOK. WITHOUT THAT, PART 1 OF THIS NOTEBOOK WILL NOT WORK.\n",
    "\n",
    "In the code-along session, we showed you how to download the daymet4 data-set for Hawaii and to create a STAC catalog stored on dCache. This enables you to access the data-set efficiently for calculation on SURF infrastructure. \n",
    "\n",
    "Here you will practice by downloading another part of the data-set corresponding to Puerto Rico, namely the day length variable, and store it in the local STAC catalog created before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import copy_asset function from stac2dcache\n",
    "# ~1 line of code\n",
    "from stac2dcache.utils import copy_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define and load catalog for Puerto Rico\n",
    "# ~2 lines of code\n",
    "catalog = pystac.Catalog.from_file('./daymet-daily-v4')\n",
    "pr = catalog.get_child(\"pr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and copy the day length variables for Puerto Rico\n",
    "# ~1 line of code\n",
    "copy_asset(\n",
    "    catalog=pr,\n",
    "    asset_key=\"dayl\",\n",
    "    update_catalog=True,\n",
    "    max_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Catalog\n",
    "# ~1 line of code\n",
    "catalog.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the Puerto Rico data-set has been downloaded and saved locally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate spring indices\n",
    "### YOU CAN RUN THIS PART INDEPENDANT OF THE PREVIOUS PART-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will calculate the leaf spring indices for Puerto-Rico for three plant species: 'Lilac', 'Arnold Red', 'Zabeli' and averaging the final results. First, the required libraries need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DO NOT MODIFY\n",
    "import dask.array as da\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import pystac\n",
    "import rioxarray\n",
    "import stac2dcache\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the (slightly modified) parameters, and functions for calculating GDH, and first leaf spring indices by running the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREDEFINED FUNCTIONS AND PARAMETERS - DO NOT MODIFY\n",
    "# Parameters and Coefficients\n",
    "BASE_TEMP_FAHRENHEIT = 31.\n",
    "\n",
    "HOURS = xr.DataArray(\n",
    "    data=da.arange(24), \n",
    "    dims=(\"hours\",),\n",
    ")\n",
    "\n",
    "DAYS = xr.DataArray(\n",
    "    data=da.arange(startdate, enddate+1),\n",
    "    dims=(\"time\",),\n",
    ")\n",
    "\n",
    "LEAF_INDEX_COEFFS = xr.DataArray(\n",
    "    data=da.from_array(\n",
    "        [\n",
    "            [3.306, 13.878, 0.201, 0.153], # Coefficients for Lilac\n",
    "            [4.266, 20.899, 0.000, 0.248], # Coefficients for Arnold Red\n",
    "            [2.802, 21.433, 0.266, 0.000], # Coefficients for Zabeli\n",
    "        ],\n",
    "        chunks=(1,-1)\n",
    "    ),\n",
    "    dims=(\"plant\", \"variable\"),\n",
    "    coords={\"plant\": [\"lilac\", \"arnold red\", \"zabelli\"]}\n",
    ")\n",
    "\n",
    "LEAF_INDEX_LIMIT = 637\n",
    "\n",
    "# Required Functions for calculations\n",
    "\n",
    "def open_dataset(urlpaths, **kwargs):\n",
    "    \"\"\"\n",
    "    Open the remote files as a single dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    ofs = fsspec.open_files(urlpaths, block_size=4*2**20)\n",
    "    return xr.open_mfdataset(\n",
    "        [of.open() for of in ofs],\n",
    "        engine=\"h5netcdf\", \n",
    "        decode_coords=\"all\",\n",
    "        drop_variables=(\"lat\", \"lon\"),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_gdh(dayl, tmin, tmax):\n",
    "    \"\"\" \n",
    "    Calculate growing degree hours (GDH). \n",
    "    \"\"\"\n",
    "    \n",
    "    dt = tmax - tmin\n",
    "    const = np.sin(np.pi/(dayl + 4) * dayl) * dt\n",
    "    \n",
    "    eq1 = np.sin(HOURS * np.pi/(dayl + 4)) * dt \n",
    "    eq2 = (1 - np.log(HOURS - np.floor(dayl))/np.log(24 - dayl)) * const\n",
    "    t = xr.where(~np.isfinite(eq2), eq1, eq2) + tmin - BASE_TEMP_FAHRENHEIT\n",
    "    t = t.clip(min=0)\n",
    "    return t.sum(dim=\"hours\", skipna=False)\n",
    "\n",
    "\n",
    "def calculate_leaf_predictors(gdh):\n",
    "    \"\"\"\n",
    "    Calculate predictors for first leaf: DDE2, DD57, MDS0, and SYNOP.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pad GDH to solve issues with first days of the year\n",
    "    gdh_padded = gdh.pad(time=(7,0), mode=\"edge\")\n",
    "    \n",
    "    # Calculating dde2 - trailing 3 days GDH sum from day i-2 to i\n",
    "    dde2 = gdh_padded.rolling(time=3, center=False).sum()\n",
    "    dde2 = dde2.isel(time=slice(7, None))  # drop padded values \n",
    "    \n",
    "    # Calculating dd57 - trailing 5-7 days GDH sum from day i-7 to i-5\n",
    "    dd57 = gdh_padded.rolling(time=8, center=False).sum() \\\n",
    "        - gdh_padded.rolling(time=5, center=False).sum()\n",
    "    dd57 = dd57.isel(time=slice(7, None))  # drop padded values\n",
    "    \n",
    "    # Calculating mds0\n",
    "    mds0 = DAYS - 1\n",
    "    \n",
    "    # Calculating synop\n",
    "    synflag = dde2>=LEAF_INDEX_LIMIT\n",
    "    synop = synflag.cumsum(dim=\"time\")\n",
    "\n",
    "    return dde2, dd57, mds0, synop\n",
    "\n",
    "\n",
    "def calculate_first_leaf(dde2, dd57, mds0, synop):\n",
    "    \"\"\"\n",
    "    Calculate day of first leaf for each plant species from GDH.\n",
    "    \"\"\" \n",
    "            \n",
    "    # Prediction calculation for first leaf\n",
    "    mdsum = LEAF_INDEX_COEFFS[:,0]*mds0 \\\n",
    "        + LEAF_INDEX_COEFFS[:,1]*synop \\\n",
    "        + LEAF_INDEX_COEFFS[:,2]*dde2 \\\n",
    "        + LEAF_INDEX_COEFFS[:,3]*dd57\n",
    "\n",
    "    mdbool = mdsum>999.5  # Calculate all occurences of first leaf\n",
    "\n",
    "    # Vectorized approach to identifying first day of leaf\n",
    "    outdate = mdbool.argmax(dim=\"time\")\n",
    "    outdate = outdate.where(mdbool.sum(dim=\"time\")>0)\n",
    "            \n",
    "    # Arnold red's first leaf is one day after reaching mdsum limit\n",
    "    day_shift = xr.DataArray(\n",
    "        da.array([0, 1, 0]),\n",
    "        dims=(\"plant\",),\n",
    "        coords={\"plant\": [\"lilac\", \"arnold red\", \"zabelli\"]}\n",
    "    )\n",
    "    outdate = outdate + day_shift\n",
    "    return outdate\n",
    "\n",
    "\n",
    "def add_mean_plant_layer(outdate):\n",
    "    \"\"\"\n",
    "    Average the spring index date over plant species and add the mean\n",
    "    as a new layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = outdate.mean(dim=\"plant\", skipna=False).round()\n",
    "    mean = mean.expand_dims(plant=[\"mean\"])\n",
    "    return xr.concat([outdate, mean], dim=\"plant\")\n",
    "\n",
    "\n",
    "def save_to_urlpath(first_leaf, urlpath, group):\n",
    "    \"\"\"\n",
    "    Save output to urlpath in Zarr format. \n",
    "    \"\"\"\n",
    "    \n",
    "    fs_map = fsspec.get_mapper(urlpath)\n",
    "    ds = xr.Dataset({\n",
    "        f\"first-leaf\": first_leaf, \n",
    "    })\n",
    "    ds.to_zarr(fs_map, mode=\"w\", group=group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and connect dask cluster\n",
    "\n",
    "Scale your system to 4 workers* and connect the dask cluster to this notebook.\n",
    "\n",
    "*PLEASE DO NOT SCALE BEYOND 4 WORKERS TO ENSURE FAIR DISTRIBUTION OF WORKLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://10.0.0.28:46245\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--DROP DASK `SLURMCluster` HERE--*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "# This defines the urlpath to the dataset stored on dCache\n",
    "\n",
    "# dCache project root path\n",
    "root_urlpath = \"dcache://pnfs/grid.sara.nl/data/remotesensing/disk/hdcrs\"\n",
    "\n",
    "# catalog path under root directory\n",
    "catalog_urlpath = f\"{root_urlpath}/daymet-daily-v4/catalog.json\"\n",
    "\n",
    "#Output path\n",
    "output_urlpath = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "# You can opt to perform the calculation for:\n",
    "#   1) One year only as in the example notebook\n",
    "#   2) For all years in the dataset from 1980 - 2022\n",
    "\n",
    "# Select year(s) for spring index calculation - use range() function for years\n",
    "# ~1 line of code\n",
    "years = range(1980,1981)\n",
    "\n",
    "# Define the day range (upto 300 days) for calculating growing degree hours\n",
    "# ~2 lines of code1\n",
    "startdate = 1\n",
    "enddate = 300\n",
    "\n",
    "# Load the catalog\n",
    "# ~1 line of code\n",
    "catalog = pystac.Catalog.from_file(catalog_urlpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess your data\n",
    "# Create here the function to preprocess the dataset which has to:\n",
    "#   1) select the predefined time range\n",
    "#   2) convert temperature to Fahrenheit\n",
    "#   3) convert daylength to hours\n",
    "# ~4-5 lines of code\n",
    "def preprocess_dataset(ds, startdate, enddate):\n",
    "    \"\"\"\n",
    "    Subset the input dataset and make necessary conversions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select time range for GDH calculation\n",
    "    ds = ds.isel(time=slice(startdate-1, enddate))\n",
    "    \n",
    "    # Convert temperatures to Fahrenheit\n",
    "    tmax = ds[\"tmax\"] * 1.8 + 32\n",
    "    tmin = ds[\"tmin\"] * 1.8 + 32\n",
    "\n",
    "    # Convert daylength from seconds to hours\n",
    "    dayl = ds[\"dayl\"] / 3600\n",
    "\n",
    "    return tmax, tmin, dayl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate leaf spring index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through years\n",
    "for year in years:\n",
    "    # Extract urlpaths to Daymet files\n",
    "    # ~2 lines of code\n",
    "    item = catalog.get_item(f\"pr-{year}\", recursive=True)\n",
    "    hrefs = [\n",
    "        item.assets[var].get_absolute_href() \n",
    "        for var in (\"tmin\", \"tmax\", \"dayl\")\n",
    "    ]\n",
    "    \n",
    "    # Open dataset using open_dataset() - think about the right chunk size\n",
    "    # ~1 line of code\n",
    "    ds = open_dataset(hrefs, chunks={\"time\": 50, \"x\": 100, \"y\": 100})\n",
    "    \n",
    "    # Preprocess the data using your preprocess function\n",
    "    # ~1 line of code\n",
    "    tmax, tmin, dayl = preprocess_dataset(ds, startdate, enddate)\n",
    "\n",
    "    # Calculate gdh using the calculate_gdh()\n",
    "    # ~1 line of code\n",
    "    gdh = calculate_gdh(dayl, tmin, tmax)\n",
    "    \n",
    "    # Rechunk gdh to an appropriate size - what is a good size for chunking?\n",
    "    # ~1 line of code\n",
    "    gdh = gdh.chunk({\"time\": enddate-startdate+1, \"x\": 100, \"y\": 100})\n",
    "    \n",
    "    # Calculate leaf spring index predictors using calculate_leaf_predictors()\n",
    "    # ~1 line of code\n",
    "    dde2, dd57, mds0, synop = calculate_leaf_predictors(gdh)\n",
    "    \n",
    "    # Calculate leaf spring index using calculate_first_leaf()\n",
    "    # ~1 line of code\n",
    "    first_leaf = calculate_first_leaf(dde2, dd57, mds0, synop)\n",
    "    \n",
    "    # Calculate the average over plants using add_mean_plant_layer() - check required inputs\n",
    "    # ~1 line of code\n",
    "    first_leaf = add_mean_plant_layer(first_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the day of first-leaf across Puerto Rico for one year \n",
    "first_leaf.plot.imshow(col=\"plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the output to file using save_to_urlpath()\n",
    "save_to_urlpath(\n",
    "    first_leaf,\n",
    "    output_urlpath, \n",
    "    f\"PuertoRico\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
